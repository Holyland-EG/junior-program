{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# План:\n",
    "1. Python\n",
    "2. Numpy\n",
    "3. Pandas\n",
    "4. Regexp + regexp with dataframes\n",
    "5. <b>Texts</b>\n",
    "6. Matplotlib, Seaborn\n",
    "7. Classification, clustering, binary, multiclass, multilabel\n",
    "8. Metrics\n",
    "9. Feature extraction\n",
    "10. Pyspark/sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXTS\n",
    "\n",
    "Рассмотрим:\n",
    "\n",
    "* предобработку текста\n",
    "* представление текста\n",
    "* понятие эмбеддинга\n",
    "* текстовую классификацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://media.giphy.com/media/nopqz91prOyvS/giphy.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Предобработка текста\n",
    "\n",
    "Текст на естественном языке, который нужно обрабатывать в задачах машинного обучения, сильно зависит от источника. Пример:\n",
    "\n",
    "Википедия\n",
    "> Литературный язык — обработанная часть общенародного языка, обладающая в большей или меньшей степени письменно закреплёнными нормами; язык всех проявлений культуры, выражающихся в словесной форме.\n",
    "\n",
    "Твиттер\n",
    "> Если у вас в компании есть люди, которые целый день сидят в чатиках и смотрят видосики, то, скорее всего, это ДАТАСАЕНТИСТЫ и у них ОБУЧАЕТСЯ\n",
    "\n",
    "Ответы@Mail.ru\n",
    "> как пишется \"Вообщем лето было отличное\" раздельно или слитно слово ВОобщем?? ?\n",
    "\n",
    "В связи с этим, возникает задача предобработки (или нормализации) текста, то есть приведения к некоторому единому виду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Приведение текста к нижнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'купил таблетки от тупости, но не смог открыть банку,ЧТО ДЕЛАТЬ???'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'купил таблетки от тупости, но не смог открыть банку,что делать???'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Удаление неинформативных символов.\n",
    "\n",
    "Такими символами могут быть символы пунктуации, спец-символы, повторяющиеся символы, цифры.\n",
    "Удалите символы пунктуации и лишние пробелы из предыдущего текста в нижнем регистре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'купил таблетки от тупостино не смог открыть банку что делать'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = re.sub('[^А-Яа-яA-Za-z ]',' ', text)\n",
    "text = re.sub('\\s{2,}','', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Разбиение текста на смысловые единицы (токенизация)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Купите кружку-термос \"Hello Kitty\" на 0.5л (64см³) за 300 рублей. До 01.01.2020.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к токенизации - это разбиение по текста по пробельным символам. \n",
    "\n",
    "**Quiz: Какая у этого подхода есть проблема?**\n",
    "\n",
    "- Объем словаря будет большим. В текстах могут встретится как слова **кружка**, **термос** по отдельности так и через тире **кружка-термос** - и при использовании простых способов векторизации текстов это будут 3 разных смысловых единицы, хотя логично было бы оставить 2.\n",
    "\n",
    "- Если перед нами стоит задача, в которой необходимо учитывать пунктуацию, то нам придется придумывать способ оторвать точки, кавычки, скобки от слов\n",
    "    \n",
    "- Если пробелы пропущены, то два слова могут остаться склееными через запятую\\точку\\дефис"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке для морфологического анализа для русского языка [`pymorphy2`](https://pymorphy2.readthedocs.io/en/latest/) есть простая вспомогательная функция для токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "\n",
    "##your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более сложной метод токенизации представлен в [`nltk`](https://www.nltk.org/): библиотеке для общего NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сравните и напишите в комментарии чем отличаются эти три метода**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка также есть новая специализированная библиотека [`razdel`](https://github.com/natasha/razdel).\n",
    "\n",
    "**Напишите функцию, которая принимает на вход текст и возвращает список токенов из метода tokenize библиотеки razdel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '\"',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " '\"',\n",
       " 'на',\n",
       " '0.5',\n",
       " 'л',\n",
       " '(',\n",
       " '64',\n",
       " 'см³',\n",
       " ')',\n",
       " 'за',\n",
       " '300',\n",
       " 'рублей',\n",
       " '.',\n",
       " 'До',\n",
       " '01.01.2020',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import tokenize\n",
    "\n",
    "def tokenize_with_razdel(text):\n",
    "    tokens = list(tokenize(text))\n",
    "    return [_.text for _ in tokens]\n",
    "\n",
    "tokenize_with_razdel(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Приведение слов к нормальной форме (стемминг, лемматизация)\n",
    "\n",
    "**Стемминг - это нормализация слова путём отбрасывания окончания по правилам языка.**\n",
    "\n",
    "Такая нормализация хорошо подходит для языков с небольшим разнообразием словоформ, например, для английского. В библиотеке nltk есть несколько реализаций стеммеров:\n",
    " - Porter stemmer\n",
    " - Snowball stemmer - только его можно использовать для русского языка (но лучше не надо)\n",
    " - Lancaster stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "SnowballStemmer(language='english').stem('running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка этот подход не очень подходит, поскольку в русском есть падежные формы, время у глаголов и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бежа'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer(language='russian').stem('бежать')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лемматизация - приведение слов к начальной морфологической форме (с помощью словаря и грамматики языка).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Две самые часто используемые библиотеки для лемматизации русских слов:\n",
    "- [pymorphy2](https://pymorphy2.readthedocs.io/en/latest/)\n",
    "- [mystem3](https://tech.yandex.ru/mystem/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к лемматизации - словарный. Здесь не учитывается контекст слова, поэтому для омонимов такой подход работает не всегда. Такой подход применяет библиотека pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "pymorphy = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Напишите функцию принимающую на вход список токенов и возвращаюшую список лемматизированных с помощью pymorphy токенов**\n",
    "\n",
    "протестируйте на примере 'на заводе стали увидел виды стали'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_text = 'на заводе стали увидел виды стали'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_pymorphy(tokens):\n",
    "    lemmas = list()\n",
    "    for t in tokens:\n",
    "        py = pymorphy.parse(t)\n",
    "        l = py[0].normal_form \n",
    "        lemmas.append(l)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на', 'завод', 'стать', 'увидеть', 'вид', 'стать']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = simple_word_tokenize(try_text)\n",
    "lemmatize_with_pymorphy(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARK: иногда чтобы сделать код проще и читаемее, можно использовать вид записи представленный ниже:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_pymorphy(tokens):\n",
    "    return [pymorphy.parse(t)[0].normal_form for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека от Яндекса `mystem3` обходит это ограничение и рассматривает контекст слова, используя статистику и правила. + Имеет свой токенизатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Напишите функцию принимающую на вход строку и возвращаюшую список лемматизированных токенов с помощью pymystem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "def lemmatize_with_mystem(text):\n",
    "    lemmas = mystem.lemmatize(text)\n",
    "    return (lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на',\n",
       " ' ',\n",
       " 'завод',\n",
       " ' ',\n",
       " 'становиться',\n",
       " ' ',\n",
       " 'увидеть',\n",
       " ' ',\n",
       " 'вид',\n",
       " ' ',\n",
       " 'становиться',\n",
       " '\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_mystem(try_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARK: Пробелы лучше дропать)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_mystem(text):\n",
    "    return [l for l in mystem.lemmatize(text)[:-1] if l!=' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на', 'завод', 'становиться', 'увидеть', 'вид', 'становиться']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_mystem(try_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще более крутая и более **медленная** библиотека `RNNMorph` базирующаяся на рекурентных сетях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\users\\cait\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\saving.py:473: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(yaml_string)\n"
     ]
    }
   ],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "\n",
    "def lemmatize_with_rnnmorph(tokens):\n",
    "    forms = predictor.predict(tokens)\n",
    "    return(forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<normal_form=на; word=на; pos=ADP; tag=_; score=1.0000>,\n",
       " <normal_form=завод; word=заводе; pos=NOUN; tag=Case=Loc|Gender=Masc|Number=Sing; score=0.9999>,\n",
       " <normal_form=стать; word=стали; pos=VERB; tag=Mood=Ind|Number=Plur|Tense=Past|VerbForm=Fin|Voice=Act; score=0.9986>,\n",
       " <normal_form=увидеть; word=увидел; pos=VERB; tag=Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act; score=1.0000>,\n",
       " <normal_form=вид; word=виды; pos=NOUN; tag=Case=Acc|Gender=Masc|Number=Plur; score=0.6663>,\n",
       " <normal_form=стать; word=стали; pos=VERB; tag=Mood=Ind|Number=Plur|Tense=Past|VerbForm=Fin|Voice=Act; score=0.8500>]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_rnnmorph(simple_word_tokenize(try_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARK: Здесь по аналогии с pymorphy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_rnnmorph(tokens):\n",
    "    return [t.normal_form for t in predictor.predict(tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на', 'завод', 'стать', 'увидеть', 'вид', 'стать']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_rnnmorph(simple_word_tokenize(try_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 One-Hot Encoding\n",
    "<img src=\"gifs/one_hot.png\" width=\"500\">\n",
    "\n",
    "При таком способе представления текстов, составляется словарь всех слов со всех документов - столбцы нашей матрицы, строки это документы. \n",
    "\n",
    "Если слово есть в документе на пересечении столбца и строки ставится 1, иначе 0. \n",
    "\n",
    "Матрица сформированная таким образом получается разреженной, т.е. если у нас очень много уникальных слов, доля единиц в строке будет маленькой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы уже получали one-hot вектора с помощью pandas\\numpy. \n",
    "Сначала нам нужно каждому слову поставить в соответствие номер, а затем перевести их в бинарные вектора. \n",
    "\n",
    "В этот раз используем библиотеку scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "words = ['What', 'the', 'hell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Получите one-hot вектора используя LabelEncoder и OneHotEncoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "int_encoded = label_encoder.fit_transform(words)\n",
    "print(int_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "oh_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "int_encoded = int_encoded.reshape(len(int_encoded), 1)\n",
    "oh_encoded = oh_encoder.fit_transform(int_encoded)\n",
    "print(oh_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Что будет, если мы сложим все one-hot вектора слов в тексте?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bag-of-words\n",
    "\n",
    "В прошлом методе, если слово употребляется в тексте на пересечении столбца-слова и строки-документа ставились 1, но теперь мы бы хотели так же знать сколько раз слово встретилось в данном документе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы посчитать количество слов в тексте, используем метод CountVectorizer из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Кот пьет молоко',\n",
    "    'Кто пьет молоко?',\n",
    "    'Молоко выпивается котом',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучите CountVectorizer на примерах, выведите вектора для трех предложений и список слов-столбцов матрицы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['выпивается', 'кот', 'котом', 'кто', 'молоко', 'пьет']\n",
      "[[0 1 0 0 1 1]\n",
      " [0 0 0 1 1 1]\n",
      " [1 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "x = CountVectorizer()\n",
    "\n",
    "count = x.fit_transform(corpus)\n",
    "\n",
    "print(x.get_feature_names())\n",
    "\n",
    "print(count.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TF-IDF\n",
    "\n",
    "**Term Frequency**  $tf(w,d)$ - сколько раз слово $w$ встретилось в документе $d$\n",
    "\n",
    "**Document Frequency** $df(w)$ - сколько документов содержат слово $w$\n",
    "\n",
    "**Inverse Document Frequency** $idf(w) = log_2(N/df(w))$  — обратная документная частотность. \n",
    "\n",
    "**TF-IDF**=$tf(w,d)*idf(w)$\n",
    "\n",
    "В гите есть презентация, с которой возможно, станет понятнее **`векторайзеры и метрики.pptx`**\n",
    "\n",
    "**Обучите TfidfVectorizer на примерах из предыдущего задания, выведите вектора для трех предложений и список слов-столбцов матрицы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['выпивается', 'кот', 'котом', 'кто', 'молоко', 'пьет']\n",
      "[[0.         0.72033345 0.         0.         0.42544054 0.54783215]\n",
      " [0.         0.         0.         0.72033345 0.42544054 0.54783215]\n",
      " [0.65249088 0.         0.65249088 0.         0.38537163 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X = tfidf.fit_transform(corpus)\n",
    "\n",
    "print(tfidf.get_feature_names())\n",
    "\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы попробуем применить описание методы предобработки и представления текста на примере анализа тональности текста. В качестве данных будем использовать небольшой датасет твитов. Всего в данных 2 класса: позитив и негатив."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Загрузка данных и получение тренировочной и тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6929, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>эти розы для прекрасной мамочки)))=_=]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>И да, у меня в этом году серьезные проблемы со...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>♥Обожаю людей, которые заставляют меня смеятьс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Вчера нашла в почтовом ящике пустую упаковку и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>очень долгожданный и хороший день был)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  positive            эти розы для прекрасной мамочки)))=_=]]\n",
       "1  negative  И да, у меня в этом году серьезные проблемы со...\n",
       "2  positive  ♥Обожаю людей, которые заставляют меня смеятьс...\n",
       "3  negative  Вчера нашла в почтовом ящике пустую упаковку и...\n",
       "4  positive             очень долгожданный и хороший день был)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разбейте выборку на две тренировочную и валидационную с помощью функции train_test_split из sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['text'], train['label'], test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверьте что доли классов на train и на test совпадают**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    0.671134\n",
       "negative    0.328866\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    0.663781\n",
       "negative    0.336219\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша выборка не сбалансирована (доля одно из класса значительно ниже доли другого), поэтому стандартные метрики качества для классификаторов вроде accuracy или roc auc нам не подходят"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужна Точность (Precision) и Полнота (Recall)!\n",
    "\n",
    "**Про эти метрики можно подробно почитать в презентации  `векторайзеры и метрики.pptx`**\n",
    "\n",
    "Для подсчета метрик будем использовать говоторые функции из sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для оценки векторизатора. В качестве модели будем использовать линейный SVM, он хорошо работает на задачах классификации текстов, когда для векторизации используются методы дающие разреженные матрицы.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Реализуйте функцию следуя инструкциям в комментариях**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def evaluate_vectorizer(vectorize, x, y):\n",
    "     \n",
    "    vectorize.fit(x)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = .3)\n",
    "    \n",
    "    vect_train = vectorize.transform(X_train)\n",
    "    vect_test = vectorize.transform(X_test)\n",
    "    \n",
    "    model = LinearSVC()\n",
    "    model.fit(vect_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(vect_test)\n",
    "    \n",
    "    #вывод метрик классификации с помощью функции classification_report - выводит precision и recall для каждого класса\n",
    "    metr = classification_report(y_test, y_pred)\n",
    "    print(metr)\n",
    "    #возвращаем предсказанные классы для теста\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Сравнение способов представления текста\n",
    "\n",
    "* необработанный текст + используйте CountVectorizer\n",
    "* переведите в lower и используйте CountVectorizer\n",
    "* lower + lemmatization + CountVectorizer\n",
    "* lower + stemming (SnowballStemer) + CountVectorizer\n",
    "Выберите лучший из двух последних ->\n",
    "* lower + lemmatization\\stemming + CountVectorizer + передавайте в векторайзер свой токенизатор (например из razdel)\n",
    "* lower + lemmatization\\stemming + CountVectorizer + передавайте в векторайзер свой токенизатор (например из razdel) + передайте список стоп слов (data/stopwords-ru.txt)\n",
    "* lower + lemmatization\\stemming + TfidfVectorizer + передавайте в векторайзер свой токенизатор (например из razdel)\n",
    "* lower + lemmatization\\stemming + TfidfVectorizer (используйте ngrams(2, 2)) + передавайте в векторайзер свой токенизатор (например из razdel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.55      0.63       677\n",
      "    positive       0.81      0.90      0.85      1402\n",
      "\n",
      "    accuracy                           0.79      2079\n",
      "   macro avg       0.77      0.73      0.74      2079\n",
      "weighted avg       0.78      0.79      0.78      2079\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'positive', ..., 'positive', 'negative',\n",
       "       'positive'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 необработанный текст + используйте CountVectorizer\n",
    "evaluate_vectorizer(CountVectorizer(), train['text'], train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 переведите в lower и используйте CountVectorizer\n",
    "import re\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('[^А-Яа-яA-Za-z ]',' ', x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('\\s{2,}',' ', x))\n",
    "train['text'] = train['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.54      0.62       703\n",
      "    positive       0.79      0.91      0.85      1376\n",
      "\n",
      "    accuracy                           0.78      2079\n",
      "   macro avg       0.77      0.72      0.74      2079\n",
      "weighted avg       0.78      0.78      0.77      2079\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['positive', 'positive', 'positive', ..., 'positive', 'positive',\n",
       "       'positive'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_vectorizer(CountVectorizer(), train['text'], train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 lower + lemmatization + CountVectorizer\n",
    "def low_lem(data):\n",
    "    tokens = tokenize_with_razdel(data)\n",
    "    lemmas = list()\n",
    "    for i in tokens:\n",
    "        py = pymorphy.parse(i)\n",
    "        l = py[0].normal_form \n",
    "        lemmas.append(l)\n",
    "    return(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text1'] = train['text'].apply(low_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>эти розы для прекрасной мамочки</td>\n",
       "      <td>этот роза для прекрасный мамочка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>и да у меня в этом году серьезные проблемы со ...</td>\n",
       "      <td>и да у я в это год серьёзный проблема с сон и ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>обожаю людей которые заставляют меня смеяться</td>\n",
       "      <td>обожать человек который заставлять я смеяться</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>вчера нашла в почтовом ящике пустую упаковку и...</td>\n",
       "      <td>вчера найти в почтовый ящик пустовать упаковка...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>очень долгожданный и хороший день был</td>\n",
       "      <td>очень долгожданный и хороший день быть</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6924</td>\n",
       "      <td>positive</td>\n",
       "      <td>ссора я я не твоя жена чтобы ты мог мне запрещ...</td>\n",
       "      <td>ссора я я не твой жена чтобы ты мочь я запреща...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6925</td>\n",
       "      <td>positive</td>\n",
       "      <td>к р а с у н я</td>\n",
       "      <td>к р а с у наш я</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6926</td>\n",
       "      <td>positive</td>\n",
       "      <td>опять улетаю сегодня в ночь до не будет в росс...</td>\n",
       "      <td>опять улетать сегодня в ночь до не быть в росс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6927</td>\n",
       "      <td>positive</td>\n",
       "      <td>хватит гарусных статусов лето пора на отдых ра...</td>\n",
       "      <td>хватить гарусный статус лето пора на отдых рас...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6928</td>\n",
       "      <td>positive</td>\n",
       "      <td>ахахахахахах л ва помнишь</td>\n",
       "      <td>ахахахахах литр ва помнить</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6929 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text  \\\n",
       "0     positive                   эти розы для прекрасной мамочки    \n",
       "1     negative  и да у меня в этом году серьезные проблемы со ...   \n",
       "2     positive     обожаю людей которые заставляют меня смеяться    \n",
       "3     negative  вчера нашла в почтовом ящике пустую упаковку и...   \n",
       "4     positive             очень долгожданный и хороший день был    \n",
       "...        ...                                                ...   \n",
       "6924  positive  ссора я я не твоя жена чтобы ты мог мне запрещ...   \n",
       "6925  positive                                      к р а с у н я   \n",
       "6926  positive  опять улетаю сегодня в ночь до не будет в росс...   \n",
       "6927  positive  хватит гарусных статусов лето пора на отдых ра...   \n",
       "6928  positive                         ахахахахахах л ва помнишь    \n",
       "\n",
       "                                                  text1  \n",
       "0                      этот роза для прекрасный мамочка  \n",
       "1     и да у я в это год серьёзный проблема с сон и ...  \n",
       "2         обожать человек который заставлять я смеяться  \n",
       "3     вчера найти в почтовый ящик пустовать упаковка...  \n",
       "4                очень долгожданный и хороший день быть  \n",
       "...                                                 ...  \n",
       "6924  ссора я я не твой жена чтобы ты мочь я запреща...  \n",
       "6925                                    к р а с у наш я  \n",
       "6926  опять улетать сегодня в ночь до не быть в росс...  \n",
       "6927  хватить гарусный статус лето пора на отдых рас...  \n",
       "6928                         ахахахахах литр ва помнить  \n",
       "\n",
       "[6929 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.60      0.66       663\n",
      "    positive       0.83      0.90      0.86      1416\n",
      "\n",
      "    accuracy                           0.80      2079\n",
      "   macro avg       0.78      0.75      0.76      2079\n",
      "weighted avg       0.80      0.80      0.80      2079\n",
      "\n",
      "['negative' 'positive' 'positive' ... 'positive' 'positive' 'positive']\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_vectorizer(CountVectorizer(), train['text1'], train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 lower + stemming (SnowballStemer) + CountVectorizer Выберите лучший из двух последни\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def low_stem(data):\n",
    "    tokens = tokenize_with_razdel(data)\n",
    "    stemming = list()\n",
    "    for i in tokens:\n",
    "        s = SnowballStemmer(language='russian').stem(i) \n",
    "        stemming.append(s)\n",
    "    return(' '.join(stemming)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text2'] = train['text'].apply(low_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.62      0.68       722\n",
      "    positive       0.81      0.89      0.85      1357\n",
      "\n",
      "    accuracy                           0.79      2079\n",
      "   macro avg       0.78      0.75      0.76      2079\n",
      "weighted avg       0.79      0.79      0.79      2079\n",
      "\n",
      "['positive' 'positive' 'positive' ... 'negative' 'positive' 'positive']\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_vectorizer(CountVectorizer(), train['text2'], train['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARK: обманываем векторайзер и подаем ему в качестве токенизатора функцию, которая еще и стемит\\лематизирует ]:>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_stem(data):\n",
    "    stemming = SnowballStemmer(language='russian')\n",
    "    return [ stemming.stem(t) for t in tokenize_with_razdel(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.62      0.67       693\n",
      "    positive       0.82      0.88      0.85      1386\n",
      "\n",
      "    accuracy                           0.79      2079\n",
      "   macro avg       0.77      0.75      0.76      2079\n",
      "weighted avg       0.79      0.79      0.79      2079\n",
      "\n",
      "['negative' 'positive' 'positive' ... 'positive' 'positive' 'positive']\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_vectorizer(CountVectorizer(tokenizer=low_stem), train['text'], train['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "стемминг по-лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.59      0.64       665\n",
      "    positive       0.82      0.88      0.85      1414\n",
      "\n",
      "    accuracy                           0.79      2079\n",
      "   macro avg       0.76      0.74      0.74      2079\n",
      "weighted avg       0.78      0.79      0.78      2079\n",
      "\n",
      "['positive' 'positive' 'positive' ... 'negative' 'positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "#5 lower + lemmatization\\stemming + CountVectorizer + передавайте в векторайзер свой токенизатор (например из razdel)\n",
    "print(evaluate_vectorizer(CountVectorizer(tokenizer = tokenize_with_razdel), train['text2'], train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c',\n",
       " 'а',\n",
       " 'алло',\n",
       " 'без',\n",
       " 'белый',\n",
       " 'близко',\n",
       " 'более',\n",
       " 'больше',\n",
       " 'большой',\n",
       " 'будем',\n",
       " 'будет',\n",
       " 'будете',\n",
       " 'будешь',\n",
       " 'будто',\n",
       " 'буду',\n",
       " 'будут',\n",
       " 'будь',\n",
       " 'бы',\n",
       " 'бывает',\n",
       " 'бывь',\n",
       " 'был',\n",
       " 'была',\n",
       " 'были',\n",
       " 'было',\n",
       " 'быть',\n",
       " 'в',\n",
       " 'важная',\n",
       " 'важное',\n",
       " 'важные',\n",
       " 'важный',\n",
       " 'вам',\n",
       " 'вами',\n",
       " 'вас',\n",
       " 'ваш',\n",
       " 'ваша',\n",
       " 'ваше',\n",
       " 'ваши',\n",
       " 'вверх',\n",
       " 'вдали',\n",
       " 'вдруг',\n",
       " 'ведь',\n",
       " 'везде',\n",
       " 'вернуться',\n",
       " 'весь',\n",
       " 'вечер',\n",
       " 'взгляд',\n",
       " 'взять',\n",
       " 'вид',\n",
       " 'видел',\n",
       " 'видеть',\n",
       " 'вместе',\n",
       " 'вне',\n",
       " 'вниз',\n",
       " 'внизу',\n",
       " 'во',\n",
       " 'вода',\n",
       " 'война',\n",
       " 'вокруг',\n",
       " 'вон',\n",
       " 'вообще',\n",
       " 'вопрос',\n",
       " 'восемнадцатый',\n",
       " 'восемнадцать',\n",
       " 'восемь',\n",
       " 'восьмой',\n",
       " 'вот',\n",
       " 'впрочем',\n",
       " 'времени',\n",
       " 'время',\n",
       " 'все',\n",
       " 'все еще',\n",
       " 'всегда',\n",
       " 'всего',\n",
       " 'всем',\n",
       " 'всеми',\n",
       " 'всему',\n",
       " 'всех',\n",
       " 'всею',\n",
       " 'всю',\n",
       " 'всюду',\n",
       " 'вся',\n",
       " 'всё',\n",
       " 'второй',\n",
       " 'вы',\n",
       " 'выйти',\n",
       " 'г',\n",
       " 'где',\n",
       " 'главный',\n",
       " 'глаз',\n",
       " 'говорил',\n",
       " 'говорит',\n",
       " 'говорить',\n",
       " 'год',\n",
       " 'года',\n",
       " 'году',\n",
       " 'голова',\n",
       " 'голос',\n",
       " 'город',\n",
       " 'да',\n",
       " 'давать',\n",
       " 'давно',\n",
       " 'даже',\n",
       " 'далекий',\n",
       " 'далеко',\n",
       " 'дальше',\n",
       " 'даром',\n",
       " 'дать',\n",
       " 'два',\n",
       " 'двадцатый',\n",
       " 'двадцать',\n",
       " 'две',\n",
       " 'двенадцатый',\n",
       " 'двенадцать',\n",
       " 'дверь',\n",
       " 'двух',\n",
       " 'девятнадцатый',\n",
       " 'девятнадцать',\n",
       " 'девятый',\n",
       " 'девять',\n",
       " 'действительно',\n",
       " 'дел',\n",
       " 'делал',\n",
       " 'делать',\n",
       " 'делаю',\n",
       " 'дело',\n",
       " 'день',\n",
       " 'деньги',\n",
       " 'десятый',\n",
       " 'десять',\n",
       " 'для',\n",
       " 'до',\n",
       " 'довольно',\n",
       " 'долго',\n",
       " 'должен',\n",
       " 'должно',\n",
       " 'должный',\n",
       " 'дом',\n",
       " 'дорога',\n",
       " 'друг',\n",
       " 'другая',\n",
       " 'другие',\n",
       " 'других',\n",
       " 'друго',\n",
       " 'другое',\n",
       " 'другой',\n",
       " 'думать',\n",
       " 'душа',\n",
       " 'е',\n",
       " 'его',\n",
       " 'ее',\n",
       " 'ей',\n",
       " 'ему',\n",
       " 'если',\n",
       " 'есть',\n",
       " 'еще',\n",
       " 'ещё',\n",
       " 'ею',\n",
       " 'её',\n",
       " 'ж',\n",
       " 'ждать',\n",
       " 'же',\n",
       " 'жена',\n",
       " 'женщина',\n",
       " 'жизнь',\n",
       " 'жить',\n",
       " 'за',\n",
       " 'занят',\n",
       " 'занята',\n",
       " 'занято',\n",
       " 'заняты',\n",
       " 'затем',\n",
       " 'зато',\n",
       " 'зачем',\n",
       " 'здесь',\n",
       " 'земля',\n",
       " 'знать',\n",
       " 'значит',\n",
       " 'значить',\n",
       " 'и',\n",
       " 'иди',\n",
       " 'идти',\n",
       " 'из',\n",
       " 'или',\n",
       " 'им',\n",
       " 'имеет',\n",
       " 'имел',\n",
       " 'именно',\n",
       " 'иметь',\n",
       " 'ими',\n",
       " 'имя',\n",
       " 'иногда',\n",
       " 'их',\n",
       " 'к',\n",
       " 'каждая',\n",
       " 'каждое',\n",
       " 'каждые',\n",
       " 'каждый',\n",
       " 'кажется',\n",
       " 'казаться',\n",
       " 'как',\n",
       " 'какая',\n",
       " 'какой',\n",
       " 'кем',\n",
       " 'книга',\n",
       " 'когда',\n",
       " 'кого',\n",
       " 'ком',\n",
       " 'комната',\n",
       " 'кому',\n",
       " 'конец',\n",
       " 'конечно',\n",
       " 'которая',\n",
       " 'которого',\n",
       " 'которой',\n",
       " 'которые',\n",
       " 'который',\n",
       " 'которых',\n",
       " 'кроме',\n",
       " 'кругом',\n",
       " 'кто',\n",
       " 'куда',\n",
       " 'лежать',\n",
       " 'лет',\n",
       " 'ли',\n",
       " 'лицо',\n",
       " 'лишь',\n",
       " 'лучше',\n",
       " 'любить',\n",
       " 'люди',\n",
       " 'м',\n",
       " 'маленький',\n",
       " 'мало',\n",
       " 'мать',\n",
       " 'машина',\n",
       " 'между',\n",
       " 'меля',\n",
       " 'менее',\n",
       " 'меньше',\n",
       " 'меня',\n",
       " 'место',\n",
       " 'миллионов',\n",
       " 'мимо',\n",
       " 'минута',\n",
       " 'мир',\n",
       " 'мира',\n",
       " 'мне',\n",
       " 'много',\n",
       " 'многочисленная',\n",
       " 'многочисленное',\n",
       " 'многочисленные',\n",
       " 'многочисленный',\n",
       " 'мной',\n",
       " 'мною',\n",
       " 'мог',\n",
       " 'могу',\n",
       " 'могут',\n",
       " 'мож',\n",
       " 'может',\n",
       " 'может быть',\n",
       " 'можно',\n",
       " 'можхо',\n",
       " 'мои',\n",
       " 'мой',\n",
       " 'мор',\n",
       " 'москва',\n",
       " 'мочь',\n",
       " 'моя',\n",
       " 'моё',\n",
       " 'мы',\n",
       " 'на',\n",
       " 'наверху',\n",
       " 'над',\n",
       " 'надо',\n",
       " 'назад',\n",
       " 'наиболее',\n",
       " 'найти',\n",
       " 'наконец',\n",
       " 'нам',\n",
       " 'нами',\n",
       " 'народ',\n",
       " 'нас',\n",
       " 'начала',\n",
       " 'начать',\n",
       " 'наш',\n",
       " 'наша',\n",
       " 'наше',\n",
       " 'наши',\n",
       " 'не',\n",
       " 'него',\n",
       " 'недавно',\n",
       " 'недалеко',\n",
       " 'нее',\n",
       " 'ней',\n",
       " 'некоторый',\n",
       " 'нельзя',\n",
       " 'нем',\n",
       " 'немного',\n",
       " 'нему',\n",
       " 'непрерывно',\n",
       " 'нередко',\n",
       " 'несколько',\n",
       " 'нет',\n",
       " 'нею',\n",
       " 'неё',\n",
       " 'ни',\n",
       " 'нибудь',\n",
       " 'ниже',\n",
       " 'низко',\n",
       " 'никакой',\n",
       " 'никогда',\n",
       " 'никто',\n",
       " 'никуда',\n",
       " 'ним',\n",
       " 'ними',\n",
       " 'них',\n",
       " 'ничего',\n",
       " 'ничто',\n",
       " 'но',\n",
       " 'новый',\n",
       " 'нога',\n",
       " 'ночь',\n",
       " 'ну',\n",
       " 'нужно',\n",
       " 'нужный',\n",
       " 'нх',\n",
       " 'о',\n",
       " 'об',\n",
       " 'оба',\n",
       " 'обычно',\n",
       " 'один',\n",
       " 'одиннадцатый',\n",
       " 'одиннадцать',\n",
       " 'однажды',\n",
       " 'однако',\n",
       " 'одного',\n",
       " 'одной',\n",
       " 'оказаться',\n",
       " 'окно',\n",
       " 'около',\n",
       " 'он',\n",
       " 'она',\n",
       " 'они',\n",
       " 'оно',\n",
       " 'опять',\n",
       " 'особенно',\n",
       " 'остаться',\n",
       " 'от',\n",
       " 'ответить',\n",
       " 'отец',\n",
       " 'откуда',\n",
       " 'отовсюду',\n",
       " 'отсюда',\n",
       " 'очень',\n",
       " 'первый',\n",
       " 'перед',\n",
       " 'писать',\n",
       " 'плечо',\n",
       " 'по',\n",
       " 'под',\n",
       " 'подойди',\n",
       " 'подумать',\n",
       " 'пожалуйста',\n",
       " 'позже',\n",
       " 'пойти',\n",
       " 'пока',\n",
       " 'пол',\n",
       " 'получить',\n",
       " 'помнить',\n",
       " 'понимать',\n",
       " 'понять',\n",
       " 'пор',\n",
       " 'пора',\n",
       " 'после',\n",
       " 'последний',\n",
       " 'посмотреть',\n",
       " 'посреди',\n",
       " 'потом',\n",
       " 'потому',\n",
       " 'почему',\n",
       " 'почти',\n",
       " 'правда',\n",
       " 'прекрасно',\n",
       " 'при',\n",
       " 'про',\n",
       " 'просто',\n",
       " 'против',\n",
       " 'процентов',\n",
       " 'путь',\n",
       " 'пятнадцатый',\n",
       " 'пятнадцать',\n",
       " 'пятый',\n",
       " 'пять',\n",
       " 'работа',\n",
       " 'работать',\n",
       " 'раз',\n",
       " 'разве',\n",
       " 'рано',\n",
       " 'раньше',\n",
       " 'ребенок',\n",
       " 'решить',\n",
       " 'россия',\n",
       " 'рука',\n",
       " 'русский',\n",
       " 'ряд',\n",
       " 'рядом',\n",
       " 'с',\n",
       " 'с кем',\n",
       " 'сам',\n",
       " 'сама',\n",
       " 'сами',\n",
       " 'самим',\n",
       " 'самими',\n",
       " 'самих',\n",
       " 'само',\n",
       " 'самого',\n",
       " 'самой',\n",
       " 'самом',\n",
       " 'самому',\n",
       " 'саму',\n",
       " 'самый',\n",
       " 'свет',\n",
       " 'свое',\n",
       " 'своего',\n",
       " 'своей',\n",
       " 'свои',\n",
       " 'своих',\n",
       " 'свой',\n",
       " 'свою',\n",
       " 'сделать',\n",
       " 'сеаой',\n",
       " 'себе',\n",
       " 'себя',\n",
       " 'сегодня',\n",
       " 'седьмой',\n",
       " 'сейчас',\n",
       " 'семнадцатый',\n",
       " 'семнадцать',\n",
       " 'семь',\n",
       " 'сидеть',\n",
       " 'сила',\n",
       " 'сих',\n",
       " 'сказал',\n",
       " 'сказала',\n",
       " 'сказать',\n",
       " 'сколько',\n",
       " 'слишком',\n",
       " 'слово',\n",
       " 'случай',\n",
       " 'смотреть',\n",
       " 'сначала',\n",
       " 'снова',\n",
       " 'со',\n",
       " 'собой',\n",
       " 'собою',\n",
       " 'советский',\n",
       " 'совсем',\n",
       " 'спасибо',\n",
       " 'спросить',\n",
       " 'сразу',\n",
       " 'стал',\n",
       " 'старый',\n",
       " 'стать',\n",
       " 'стол',\n",
       " 'сторона',\n",
       " 'стоять',\n",
       " 'страна',\n",
       " 'суть',\n",
       " 'считать',\n",
       " 'т',\n",
       " 'та',\n",
       " 'так',\n",
       " 'такая',\n",
       " 'также',\n",
       " 'таки',\n",
       " 'такие',\n",
       " 'такое',\n",
       " 'такой',\n",
       " 'там',\n",
       " 'твои',\n",
       " 'твой',\n",
       " 'твоя',\n",
       " 'твоё',\n",
       " 'те',\n",
       " 'тебе',\n",
       " 'тебя',\n",
       " 'тем',\n",
       " 'теми',\n",
       " 'теперь',\n",
       " 'тех',\n",
       " 'то',\n",
       " 'тобой',\n",
       " 'тобою',\n",
       " 'товарищ',\n",
       " 'тогда',\n",
       " 'того',\n",
       " 'тоже',\n",
       " 'только',\n",
       " 'том',\n",
       " 'тому',\n",
       " 'тот',\n",
       " 'тою',\n",
       " 'третий',\n",
       " 'три',\n",
       " 'тринадцатый',\n",
       " 'тринадцать',\n",
       " 'ту',\n",
       " 'туда',\n",
       " 'тут',\n",
       " 'ты',\n",
       " 'тысяч',\n",
       " 'у',\n",
       " 'увидеть',\n",
       " 'уж',\n",
       " 'уже',\n",
       " 'улица',\n",
       " 'уметь',\n",
       " 'утро',\n",
       " 'хороший',\n",
       " 'хорошо',\n",
       " 'хотел бы',\n",
       " 'хотеть',\n",
       " 'хоть',\n",
       " 'хотя',\n",
       " 'хочешь',\n",
       " 'час',\n",
       " 'часто',\n",
       " 'часть',\n",
       " 'чаще',\n",
       " 'чего',\n",
       " 'человек',\n",
       " 'чем',\n",
       " 'чему',\n",
       " 'через',\n",
       " 'четвертый',\n",
       " 'четыре',\n",
       " 'четырнадцатый',\n",
       " 'четырнадцать',\n",
       " 'что',\n",
       " 'чтоб',\n",
       " 'чтобы',\n",
       " 'чуть',\n",
       " 'шестнадцатый',\n",
       " 'шестнадцать',\n",
       " 'шестой',\n",
       " 'шесть',\n",
       " 'эта',\n",
       " 'эти',\n",
       " 'этим',\n",
       " 'этими',\n",
       " 'этих',\n",
       " 'это',\n",
       " 'этого',\n",
       " 'этой',\n",
       " 'этом',\n",
       " 'этому',\n",
       " 'этот',\n",
       " 'эту',\n",
       " 'я',\n",
       " 'являюсь']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6 lower + lemmatization\\stemming + CountVectorizer + передавайте в векторайзер свой токенизатор (например из razdel) \n",
    "# + передайте список стоп слов (data/stopwords-ru.txt)\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "f = open('data/stopwords-ru.txt', 'r', encoding='utf8')\n",
    "stop_w = [line.strip() for line in f]\n",
    "stop_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\users\\cait\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.59      0.64       691\n",
      "    positive       0.81      0.88      0.84      1388\n",
      "\n",
      "    accuracy                           0.78      2079\n",
      "   macro avg       0.76      0.74      0.74      2079\n",
      "weighted avg       0.78      0.78      0.78      2079\n",
      "\n",
      "['positive' 'positive' 'positive' ... 'positive' 'positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_vectorizer(CountVectorizer(tokenizer = tokenize_with_razdel, stop_words = stop_w), train['text2'], train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.62      0.69       703\n",
      "    positive       0.82      0.91      0.87      1376\n",
      "\n",
      "    accuracy                           0.81      2079\n",
      "   macro avg       0.80      0.76      0.78      2079\n",
      "weighted avg       0.81      0.81      0.81      2079\n",
      "\n",
      "['positive' 'positive' 'positive' ... 'negative' 'negative' 'positive']\n"
     ]
    }
   ],
   "source": [
    "#7 lower + lemmatization\\stemming + TfidfVectorizer + передавайте в векторайзер свой токенизатор (например из razdel)\n",
    "print(evaluate_vectorizer(TfidfVectorizer(tokenizer = tokenize_with_razdel), train['text2'], train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.44      0.57       695\n",
      "    positive       0.77      0.95      0.85      1384\n",
      "\n",
      "    accuracy                           0.78      2079\n",
      "   macro avg       0.80      0.70      0.71      2079\n",
      "weighted avg       0.79      0.78      0.76      2079\n",
      "\n",
      "['positive' 'positive' 'negative' ... 'positive' 'positive' 'positive']\n"
     ]
    }
   ],
   "source": [
    "#8 lower + lemmatization\\stemming + TfidfVectorizer (используйте ngrams(2, 2)) \n",
    "# + передавайте в векторайзер свой токенизатор (например из razdel)\n",
    "print(evaluate_vectorizer(TfidfVectorizer(tokenizer = tokenize_with_razdel, ngram_range = (1,3)), train['text2'], train['label']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
